{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preliminar setup</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import ast \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "european = ['^SPX', '^NDX', '^RUT']\n",
    "#european = ['^NDX']\n",
    "\n",
    "american = ['NVDA', 'JNJ', 'XOM']\n",
    "\n",
    "#Parametric string\n",
    "opt_filename = './data/options_daily/raw/{date_dir}/{date_file}_{title}_{type}.csv'\n",
    "\n",
    "opt_filename_proc = './data/options_daily/proc/{date_dir}/{date_file}_{title}_{type}.csv'\n",
    "\n",
    "title_filename = './data/title/{title}.csv'\n",
    "\n",
    "#List of dates day by day from 2024_11_12 to 2024_11_29 \n",
    "dates = pd.date_range(start='2024-11-11', end='2024-11-29').strftime('%Y_%m_%d').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scrape title data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_title_data(title, start_date, end_date):\n",
    "    stock = yf.Ticker(title)\n",
    "    historical_data = stock.history(start=start_date, end=end_date)\n",
    "    #Add column log ret given by ln(close_price(t))-ln(close_price(t-1))\n",
    "    historical_data['log_ret'] = np.log(historical_data['Close']) - np.log(historical_data['Close'].shift(1))\n",
    "    #remove first row\n",
    "    historical_data = historical_data.iloc[1:]\n",
    "    historical_data.to_csv(title_filename.format(title=title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2004-11-29\"\n",
    "end_date = \"2024-12-02\"\n",
    "\n",
    "for title in american + european:\n",
    "    print(f\"Scraping {title}\")\n",
    "    scrape_title_data(title, start_date, end_date)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scrape Options Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_options_data(options, today):\n",
    "    \n",
    "    for idx in options:\n",
    "        spx = yf.Ticker(idx)\n",
    "\n",
    "        # get option chain for specific expiration\n",
    "        try:\n",
    "            opt = spx.option_chain('0000-00-00')\n",
    "        except Exception as e:\n",
    "            list_string = \"[\" + str(e).split('[')[1]\n",
    "            list_string = list_string.replace(\" \", \"\")\n",
    "            list_string = list_string.replace(\",\", \"','\")\n",
    "            list_string = list_string.replace(\"[\", \"['\")\n",
    "            list_string = list_string.replace(\"]\", \"']\")\n",
    "            option_dates = ast.literal_eval(list_string)\n",
    "        \n",
    "        all_calls = pd.DataFrame()\n",
    "        all_puts = pd.DataFrame()\n",
    "        \n",
    "        # Define the cutoff date\n",
    "        cutoff_date = datetime(2024, 12, 31)\n",
    "        \n",
    "        for date in option_dates:\n",
    "            # Convert date to a datetime object if it's not already one\n",
    "            if isinstance(date, str):\n",
    "                date_obj = datetime.strptime(date, '%Y-%m-%d')\n",
    "            \n",
    "            if date_obj < cutoff_date:\n",
    "                opt = spx.option_chain(date)\n",
    "                \n",
    "                #Process calls\n",
    "                call = opt.calls\n",
    "                call['expiration_date'] = date #add expiration date to the dataframe\n",
    "                all_calls = pd.concat([all_calls, call], ignore_index=True)\n",
    "                #all_calls = all_calls[all_calls.isna().sum(axis=1) <= 1]\n",
    "                #all_calls = all_calls.dropna()\n",
    "                \n",
    "                #Process puts\n",
    "                put = opt.puts\n",
    "                put['expiration_date'] = date #add expiration_date to the dataframe\n",
    "                all_puts = pd.concat([all_puts, put], ignore_index=True)\n",
    "                #all_puts = all_puts[all_puts.isna().sum(axis=1) <= 1]\n",
    "                #all_puts = all_puts.dropna()\n",
    "        \n",
    "        #If doesn't exist, create a data folder\n",
    "        all_calls.to_csv('./data/options_daily/raw/' + today + '/' + today + '_' + idx + '_calls.csv', index=False)\n",
    "        all_puts.to_csv('./data/options_daily/raw/' + today + '/' + today + '_' + idx + '_puts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get today date in format yyyy_mm_dd\n",
    "today = pd.Timestamp.today().strftime('%Y_%m_%d')\n",
    "\n",
    "try:\n",
    "    os.makedirs('./data/options_daily/raw/' + today)\n",
    "except Exception as e:\n",
    "    print('Data already written for today')\n",
    "    exit()\n",
    "\n",
    "print('Scraping European options data')\n",
    "scrape_options_data(european, today)\n",
    "\n",
    "print('Scraping American options data')\n",
    "scrape_options_data(american, today)\n",
    "    \n",
    "print('Scraping completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Take only data until 29/11/2024 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all datasets take only the rows with expiration_date until 2024-11-29\n",
    "for date in dates:\n",
    "    for idx in european + american:\n",
    "        for option_type in ['calls', 'puts']:\n",
    "            df = pd.read_csv(opt_filename.format(date_dir=date, date_file=date, title=idx, type=option_type))\n",
    "            df['expiration_date'] = pd.to_datetime(df['expiration_date'])\n",
    "            df = df[df['expiration_date'] <= '2024-11-29']\n",
    "            df.to_csv(opt_filename_proc.format(date_dir=date, date_file=date, title=idx, type=option_type), index=False)\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Take for every day only the expiration dates contained in all files (Intersection)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "dates_lists = []\n",
    "\n",
    "#Build a list made of a list for every day sampled, for each day build a list, for each title, containing all the expiration dates for that title\n",
    "for date in dates:\n",
    "    day_dates_lists = []\n",
    "    for idx in european + american:\n",
    "        option_dates_list = []\n",
    "        for option_type in ['calls', 'puts']:\n",
    "            df = pd.read_csv(opt_filename_proc.format(date_dir=date, date_file=date, title=idx, type=option_type))\n",
    "            option_dates_list.extend(df['expiration_date'].unique())\n",
    "        day_dates_lists.append(set(option_dates_list)) #Set to remove duplicates from the add of the same dates in put and call dataset\n",
    "            \n",
    "    dates_lists.append(day_dates_lists)\n",
    "\n",
    "#Note that there is no 2024-11-28 cause it's Thanksgiving\n",
    "#print(len(dates_lists)) #Expected 19 as we have sampled 19 days\n",
    "#print(len(dates_lists[0])) #Expected 6 as we have 6 titles\n",
    "#print(len(dates_lists[0][0])) #Expected X\n",
    "\n",
    "#Now take the intersection of the expiration dates for each title in each day\n",
    "intersection_lists = []\n",
    "for day_lists in dates_lists:\n",
    "    intersection = list(reduce(lambda x, y: set(x) & set(y), day_lists))\n",
    "    print(len(intersection))\n",
    "    intersection_lists.append(intersection)\n",
    "print(len(intersection_lists)) #Expected 19 as we have sampled 19 days\n",
    "print(intersection_lists)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Take only data in intersection list of erxpirations</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all datasets take only the dates in the instersection list\n",
    "for i in range(0, len(dates)):\n",
    "    for idx in european + american:\n",
    "        for option_type in ['calls', 'puts']:\n",
    "            df = pd.read_csv(opt_filename_proc.format(date_dir=dates[i], date_file=dates[i], title=idx, type=option_type))\n",
    "            df = df[df['expiration_date'].isin(intersection_lists[i])]\n",
    "            df.to_csv(opt_filename_proc.format(date_dir=dates[i], date_file=dates[i], title=idx, type=option_type), index=False)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Take for each day only the put and calls with the same last trade</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Calcolo del tasso privo di rischio</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divido i dataset per anno\n",
    "\n",
    "dataset_2002_2023 = './data/bond/daily-treasury-rates_2002-2023.csv'\n",
    "dataset_2024 = './data/bond/daily-treasury-rates_2024.csv'\n",
    "\n",
    "#From the dataset 2002_2003 create a dataset for each year and save it in the same folder\n",
    "df = pd.read_csv(dataset_2002_2023)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['year'] = df['Date'].dt.year\n",
    "#Create a dataset for each year fill the empty values with NaN\n",
    "for year in range(2002, 2024):\n",
    "    df_year = df[df['year'] == year]\n",
    "    #remove the column year\n",
    "    df_year = df_year.drop(columns=['year'])\n",
    "    df_year.to_csv(f'./data/bond/daily-treasury-rates_{year}.csv', index=False, na_rep='NaN')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcolo la media dei rendimenti\n",
    "\n",
    "tb_dates = ['2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022', '2023', '2024']\n",
    "\n",
    "#create a dateset concatenating each year\n",
    "df_all = pd.DataFrame()\n",
    "for date in tb_dates:\n",
    "    df = pd.read_csv(f'./data/bond/daily-treasury-rates_{date}.csv')\n",
    "    df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "#drop date column\n",
    "df_all = df_all.drop(columns=['Date'])\n",
    "\n",
    "#Take mean skipping nan values\n",
    "means = df_all.mean(skipna=True)\n",
    "\n",
    "df_means = pd.DataFrame(means).T\n",
    "\n",
    "#output to csv\n",
    "df_means.to_csv('./data/bond/daily-treasury-rates_means.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Calcolo della volatilitá di lungo periodo</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Series Initialization:\n",
      " ARMA Model:                arma\n",
      " Formula Mean:              ~ arma(0, 0)\n",
      " GARCH Model:               garch\n",
      " Formula Variance:          ~ garch(1, 1)\n",
      " ARMA Order:                0 0\n",
      " Max ARMA Order:            0\n",
      " GARCH Order:               1 1\n",
      " Max GARCH Order:           1\n",
      " Maximum Order:             1\n",
      " Conditional Dist:          norm\n",
      " h.start:                   2\n",
      " llh.start:                 1\n",
      " Length of Series:          5014\n",
      " Recursion Init:            mci\n",
      " Series Scale:             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.01210523\n",
      "\n",
      "Parameter Initialization:\n",
      " Initial Parameters:          $params\n",
      " Limits of Transformations:   $U, $V\n",
      " Which Parameters are Fixed?  $includes\n",
      " Parameter Matrix:\n",
      "                     U           V     params includes\n",
      "    mu     -0.26293673   0.2629367 0.02629367     TRUE\n",
      "    omega   0.00000100 100.0000000 0.10000000     TRUE\n",
      "    alpha1  0.00000001   1.0000000 0.10000000     TRUE\n",
      "    gamma1 -0.99999999   1.0000000 0.10000000    FALSE\n",
      "    beta1   0.00000001   1.0000000 0.80000000     TRUE\n",
      "    delta   0.00000000   2.0000000 2.00000000    FALSE\n",
      "    skew    0.10000000  10.0000000 1.00000000    FALSE\n",
      "    shape   1.00000000  10.0000000 4.00000000    FALSE\n",
      " Index List of Parameters to be Optimized:\n",
      "    mu  omega alpha1  beta1 \n",
      "     1      2      3      5 \n",
      " Persistence:                  0.9 \n",
      "\n",
      "\n",
      "--- START OF TRACE ---\n",
      "Selected Algorithm: nlminb \n",
      "\n",
      "R coded nlminb Solver: \n",
      "\n",
      "  0:     6019.3330: 0.0262937 0.100000 0.100000 0.800000\n",
      "  1:     5869.8823: 0.0262943 0.0730638 0.0985277 0.785939\n",
      "  2:     5796.4570: 0.0262959 0.0460503 0.112471 0.784799\n",
      "  3:     5780.2522: 0.0262980 0.0524643 0.136647 0.802114\n",
      "  4:     5735.8812: 0.0263056 0.0241544 0.147137 0.798388\n",
      "  5:     5715.6761: 0.0263627 0.0246721 0.169443 0.818952\n",
      "  6:     5714.8262: 0.0264577 0.0118529 0.154738 0.842013\n",
      "  7:     5703.9975: 0.0266107 0.0199717 0.146407 0.834128\n",
      "  8:     5703.0915: 0.0267142 0.0179303 0.134344 0.842262\n",
      "  9:     5703.0668: 0.0268119 0.0178311 0.125385 0.853980\n",
      " 10:     5702.6104: 0.0270090 0.0166403 0.125323 0.853519\n",
      " 11:     5702.3893: 0.0271632 0.0175700 0.128465 0.849956\n",
      " 12:     5701.4528: 0.0303564 0.0172361 0.134700 0.845518\n",
      " 13:     5699.5164: 0.0372700 0.0171925 0.131172 0.850501\n",
      " 14:     5698.6319: 0.0441825 0.0186374 0.130103 0.843071\n",
      " 15:     5698.3462: 0.0441827 0.0198537 0.130769 0.843861\n",
      " 16:     5698.0871: 0.0442189 0.0190469 0.130772 0.843889\n",
      " 17:     5697.9672: 0.0442198 0.0189786 0.131922 0.844993\n",
      " 18:     5697.8697: 0.0442568 0.0182430 0.131856 0.844867\n",
      " 19:     5697.8257: 0.0442980 0.0184704 0.131983 0.845032\n",
      " 20:     5697.8040: 0.0443380 0.0181098 0.132221 0.845242\n",
      " 21:     5697.7822: 0.0443795 0.0182900 0.132311 0.845358\n",
      " 22:     5697.7716: 0.0444214 0.0181721 0.132342 0.845375\n",
      " 23:     5697.7621: 0.0444632 0.0182193 0.132418 0.845459\n",
      " 24:     5697.7531: 0.0445051 0.0181270 0.132438 0.845466\n",
      " 25:     5697.0498: 0.0510784 0.0186115 0.132623 0.844092\n",
      " 26:     5696.9215: 0.0527193 0.0175061 0.130544 0.848529\n",
      " 27:     5696.7819: 0.0561688 0.0179359 0.134479 0.844515\n",
      " 28:     5696.7699: 0.0570394 0.0181068 0.133429 0.844983\n",
      " 29:     5696.7690: 0.0568713 0.0180662 0.133626 0.844906\n",
      " 30:     5696.7690: 0.0568700 0.0180656 0.133622 0.844910\n",
      "\n",
      "Final Estimate of the Negative LLH:\n",
      " LLH:  -16435.62    norm LLH:  -3.277946 \n",
      "          mu        omega       alpha1        beta1 \n",
      "6.884242e-04 2.647272e-06 1.336223e-01 8.449097e-01 \n",
      "\n",
      "R-optimhess Difference Approximated Hessian Matrix:\n",
      "                  mu         omega        alpha1         beta1\n",
      "mu      -88043370.59 -2.787676e+08  5.138873e+04 -3.776831e+04\n",
      "omega  -278767605.74 -3.799015e+13 -1.161722e+09 -1.849633e+09\n",
      "alpha1      51388.73 -1.161722e+09 -7.937572e+04 -9.448616e+04\n",
      "beta1      -37768.31 -1.849633e+09 -9.448616e+04 -1.312812e+05\n",
      "attr(,\"time\")\n",
      "Time difference of 0.0504415 secs\n",
      "\n",
      "--- END OF TRACE ---\n",
      "\n",
      "\n",
      "Time to Estimate Parameters:\n",
      " Time difference of 0.4578342 secs\n",
      "MU -> 0.0006884241705465155\n",
      "OMEGA -> 2.6472717873157712e-06\n",
      "ALPHA1 -> 0.13362232446743236\n",
      "BETA1 -> 0.8449096685230773\n"
     ]
    }
   ],
   "source": [
    "#Dato che i titoli nel mercato sono autocorrelati e eteroschedastici cioé hanno varianza variabile posso usare un modello garch per calcolare la volatilitá di lungo periodo\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "# Import the fGarch library in R\n",
    "ro.r(\"\"\"\n",
    "if (!require(fGarch)) install.packages(\"fGarch\", repos=\"http://cran.r-project.org\")\n",
    "library(fGarch)\n",
    "\n",
    "# Carica anche i pacchetti richiesti per ridurre l'avviso\n",
    "if (!require(fBasics)) install.packages(\"fBasics\", repos=\"http://cran.r-project.org\")\n",
    "if (!require(timeDate)) install.packages(\"timeDate\", repos=\"http://cran.r-project.org\")\n",
    "if (!require(timeSeries)) install.packages(\"timeSeries\", repos=\"http://cran.r-project.org\")\n",
    "\n",
    "library(fBasics)\n",
    "library(timeDate)\n",
    "library(timeSeries)\n",
    "\"\"\")\n",
    "\n",
    "pandas2ri.activate()\n",
    "\n",
    "european = ['^SPX']\n",
    "\n",
    "american = []\n",
    "\n",
    "for title in european + american:\n",
    "    df = pd.read_csv(title_filename.format(title=title))\n",
    "    \n",
    "    # Prendo come training set tutti i dati fino al 31 ottobre 2024\n",
    "    r_data_training = df[df['Date'] <= '2024-10-31']['log_ret']\n",
    "\n",
    "    # Converti la Serie Pandas in un DataFrame per facilitarne la conversione in R\n",
    "    r_data_training = pd.DataFrame(r_data_training, columns=[\"log_ret\"])\n",
    "\n",
    "    # Converti il DataFrame Pandas in un oggetto R\n",
    "    r_data_training = pandas2ri.py2rpy(r_data_training)\n",
    "\n",
    "    # Passa il dato a R\n",
    "    ro.globalenv['returns'] = r_data_training\n",
    "\n",
    "    # Scrivi lo script per calcolare il modello GARCH con la serie reale\n",
    "    r_script = \"\"\"\n",
    "    library(fGarch)\n",
    "\n",
    "    # Fit GARCH(1,1) con i dati reali\n",
    "    garch_model <- garchFit(~garch(1, 1), data = returns)\n",
    "\n",
    "    # Estrai i coefficienti\n",
    "    coefficients <- coef(garch_model)\n",
    "\n",
    "    # Restituisci i coefficienti\n",
    "    coefficients\n",
    "    \"\"\"\n",
    "\n",
    "    # Esecuzione del codice in R\n",
    "    ro.r(r_script)\n",
    "\n",
    "    # Recupera i risultati da R\n",
    "    coefficients = ro.r('coefficients')\n",
    "    #Returns mu, omega, alpha1, beta1\n",
    "    print(\"MU -> \" + str(coefficients[0]))\n",
    "    print(\"OMEGA -> \" + str(coefficients[1]))\n",
    "    print(\"ALPHA1 -> \" + str(coefficients[2]))\n",
    "    print(\"BETA1 -> \" + str(coefficients[3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
