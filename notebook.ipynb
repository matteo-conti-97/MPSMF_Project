{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preliminar setup</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import ast \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "european = ['^SPX', '^NDX', '^RUT']\n",
    "#european = ['^NDX']\n",
    "\n",
    "american = ['NVDA', 'JNJ', 'XOM']\n",
    "\n",
    "#Parametric string\n",
    "opt_filename = './data/options_daily/raw/{date_dir}/{date_file}_{title}_{type}.csv'\n",
    "\n",
    "opt_filename_proc = './data/options_daily/proc/{date_dir}/{date_file}_{title}_{type}.csv'\n",
    "\n",
    "title_filename = './data/title/{title}.csv'\n",
    "\n",
    "#List of dates day by day from 2024_11_12 to 2024_11_29 \n",
    "dates = pd.date_range(start='2024-11-11', end='2024-11-29').strftime('%Y_%m_%d').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scrape title data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_title_data(title, start_date, end_date):\n",
    "    stock = yf.Ticker(title)\n",
    "    historical_data = stock.history(start=start_date, end=end_date)\n",
    "    #Add column log ret given by ln(close_price(t))-ln(close_price(t-1))\n",
    "    historical_data['log_ret'] = np.log(historical_data['Close']) - np.log(historical_data['Close'].shift(1))\n",
    "    #remove first row\n",
    "    historical_data = historical_data.iloc[1:]\n",
    "    historical_data.to_csv(title_filename.format(title=title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2021-12-01\"\n",
    "end_date = \"2024-12-02\"\n",
    "\n",
    "for title in american + european:\n",
    "    print(f\"Scraping {title}\")\n",
    "    scrape_title_data(title, start_date, end_date)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Scrape Options Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_options_data(options, today):\n",
    "    \n",
    "    for idx in options:\n",
    "        spx = yf.Ticker(idx)\n",
    "\n",
    "        # get option chain for specific expiration\n",
    "        try:\n",
    "            opt = spx.option_chain('0000-00-00')\n",
    "        except Exception as e:\n",
    "            list_string = \"[\" + str(e).split('[')[1]\n",
    "            list_string = list_string.replace(\" \", \"\")\n",
    "            list_string = list_string.replace(\",\", \"','\")\n",
    "            list_string = list_string.replace(\"[\", \"['\")\n",
    "            list_string = list_string.replace(\"]\", \"']\")\n",
    "            option_dates = ast.literal_eval(list_string)\n",
    "        \n",
    "        all_calls = pd.DataFrame()\n",
    "        all_puts = pd.DataFrame()\n",
    "        \n",
    "        # Define the cutoff date\n",
    "        cutoff_date = datetime(2024, 12, 31)\n",
    "        \n",
    "        for date in option_dates:\n",
    "            # Convert date to a datetime object if it's not already one\n",
    "            if isinstance(date, str):\n",
    "                date_obj = datetime.strptime(date, '%Y-%m-%d')\n",
    "            \n",
    "            if date_obj < cutoff_date:\n",
    "                opt = spx.option_chain(date)\n",
    "                \n",
    "                #Process calls\n",
    "                call = opt.calls\n",
    "                call['expiration_date'] = date #add expiration date to the dataframe\n",
    "                all_calls = pd.concat([all_calls, call], ignore_index=True)\n",
    "                #all_calls = all_calls[all_calls.isna().sum(axis=1) <= 1]\n",
    "                #all_calls = all_calls.dropna()\n",
    "                \n",
    "                #Process puts\n",
    "                put = opt.puts\n",
    "                put['expiration_date'] = date #add expiration_date to the dataframe\n",
    "                all_puts = pd.concat([all_puts, put], ignore_index=True)\n",
    "                #all_puts = all_puts[all_puts.isna().sum(axis=1) <= 1]\n",
    "                #all_puts = all_puts.dropna()\n",
    "        \n",
    "        #If doesn't exist, create a data folder\n",
    "        all_calls.to_csv('./data/options_daily/raw/' + today + '/' + today + '_' + idx + '_calls.csv', index=False)\n",
    "        all_puts.to_csv('./data/options_daily/raw/' + today + '/' + today + '_' + idx + '_puts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get today date in format yyyy_mm_dd\n",
    "today = pd.Timestamp.today().strftime('%Y_%m_%d')\n",
    "\n",
    "try:\n",
    "    os.makedirs('./data/options_daily/raw/' + today)\n",
    "except Exception as e:\n",
    "    print('Data already written for today')\n",
    "    exit()\n",
    "\n",
    "print('Scraping European options data')\n",
    "scrape_options_data(european, today)\n",
    "\n",
    "print('Scraping American options data')\n",
    "scrape_options_data(american, today)\n",
    "    \n",
    "print('Scraping completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Take only data until 29/11/2024 </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all datasets take only the rows with expiration_date until 2024-11-29\n",
    "for date in dates:\n",
    "    for idx in european + american:\n",
    "        for option_type in ['calls', 'puts']:\n",
    "            df = pd.read_csv(opt_filename.format(date_dir=date, date_file=date, title=idx, type=option_type))\n",
    "            df['expiration_date'] = pd.to_datetime(df['expiration_date'])\n",
    "            df = df[df['expiration_date'] <= '2024-11-29']\n",
    "            df.to_csv(opt_filename_proc.format(date_dir=date, date_file=date, title=idx, type=option_type), index=False)\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Take for every day and for every title the intesection in the couple (put, call) of strike K, expiration date and last trade date</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "for date in dates:\n",
    "    for title in american + european:\n",
    "        call_df = pd.read_csv(opt_filename.format(date_dir=date, date_file=date, title=title, type='calls'))\n",
    "        put_df = pd.read_csv(opt_filename.format(date_dir=date, date_file=date, title=title, type='puts'))\n",
    "        \n",
    "        #Make lastTradeDate from yyyy-mm-dd hh:mm:ss to yyyy-mm-dd\n",
    "        call_df['lastTradeDate'] = call_df['lastTradeDate'].str.split(' ').str[0]\n",
    "        put_df['lastTradeDate'] = put_df['lastTradeDate'].str.split(' ').str[0]\n",
    "        \n",
    "        #Extract unique values for strikes and expirations\n",
    "        call_strikes = call_df['strike'].unique()\n",
    "        call_expirations = call_df['expiration_date'].unique()\n",
    "        \n",
    "        put_strikes = put_df['strike'].unique()\n",
    "        put_expirations = put_df['expiration_date'].unique()\n",
    "        \n",
    "        '''Extract call and puts with same lastTradeDate or at most lastTradeDate +- 2 days\n",
    "           so if i have the call_last_trade_dates=[2024-11-12, 2024-11-12, 2024-11-14, 2024-11-15] and \n",
    "           the put_last_trade_dates=[2024-11-12, 2024-11-12, 2024-11-12, 2024-11-12] i take \n",
    "           [2024-11-12, 2024-11-14]\n",
    "        '''\n",
    "        \n",
    "        call_last_trade_dates = call_df['lastTradeDate'].unique()\n",
    "        put_last_trade_dates = put_df['lastTradeDate'].unique()\n",
    "        \n",
    "        call_last_trade_dates = pd.to_datetime(call_last_trade_dates)\n",
    "        put_last_trade_dates = pd.to_datetime(put_last_trade_dates)\n",
    "\n",
    "        # Extract call and puts with same lastTradeDate or at most lastTradeDate +- 2 days\n",
    "        common_trade_dates = []\n",
    "        for trade_date in put_last_trade_dates:\n",
    "            if any((call_last_trade_dates >= trade_date - timedelta(days=2)) & (call_last_trade_dates <= trade_date + timedelta(days=2))):\n",
    "                common_trade_dates.append(trade_date)\n",
    "\n",
    "        # Convert common_trade_dates back to string format if needed\n",
    "        common_trade_dates = [trade_date.strftime('%Y-%m-%d') for trade_date in common_trade_dates]        \n",
    "          \n",
    "        #Extract the common values between calls and puts\n",
    "        strikes = np.intersect1d(call_strikes, put_strikes)\n",
    "        expirations = np.intersect1d(call_expirations, put_expirations)        \n",
    "        \n",
    "                    \n",
    "        #Filter the dataframes\n",
    "        filtered_call_df = call_df[call_df['strike'].isin(strikes) & \n",
    "                          call_df['expiration_date'].isin(expirations) & \n",
    "                          call_df['lastTradeDate'].isin(common_trade_dates)]\n",
    "        \n",
    "        filtered_put_df = put_df[put_df['strike'].isin(strikes) &\n",
    "                        put_df['expiration_date'].isin(expirations) &\n",
    "                        put_df['lastTradeDate'].isin(common_trade_dates)]\n",
    "        \n",
    "                \n",
    "           \n",
    "        \n",
    "        #Save the dataframes\n",
    "        filtered_call_df.to_csv(opt_filename_proc.format(date_dir=date, date_file=date, title=title, type='calls'), index=False)\n",
    "        filtered_put_df.to_csv(opt_filename_proc.format(date_dir=date, date_file=date, title=title, type='puts'), index=False)\n",
    "        \n",
    "print(\"Done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Calcolo del tasso privo di rischio</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcolo la media dei rendimenti\n",
    "\n",
    "tb_dates = ['2024']\n",
    "\n",
    "#create a dateset concatenating each year\n",
    "df_all = pd.DataFrame()\n",
    "for date in tb_dates:\n",
    "    df = pd.read_csv(f'./data/bond/daily-treasury-rates_{date}.csv')\n",
    "    df_all = pd.concat([df_all, df], ignore_index=True)\n",
    "\n",
    "#Take only the elements in Date column which starts with 11/\n",
    "df_all = df_all[df_all['Date'].str.startswith('11/')]\n",
    "\n",
    "#drop date column\n",
    "df_all = df_all.drop(columns=['Date'])\n",
    "\n",
    "#Take mean skipping nan values\n",
    "means = df_all.mean(skipna=True)\n",
    "\n",
    "df_means = pd.DataFrame(means).T\n",
    "\n",
    "#output to csv\n",
    "df_means.to_csv('./data/bond/daily-treasury-rates_mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Calcolo della volatilitá di lungo periodo</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dato che i titoli nel mercato sono autocorrelati e eteroschedastici cioé hanno varianza variabile posso usare un modello garch per calcolare la volatilitá di lungo periodo\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "\n",
    "# Import the fGarch library in R\n",
    "ro.r(\"\"\"\n",
    "if (!require(fGarch)) install.packages(\"fGarch\", repos=\"http://cran.r-project.org\")\n",
    "library(fGarch)\n",
    "\n",
    "# Carica anche i pacchetti richiesti per ridurre l'avviso\n",
    "if (!require(fBasics)) install.packages(\"fBasics\", repos=\"http://cran.r-project.org\")\n",
    "if (!require(timeDate)) install.packages(\"timeDate\", repos=\"http://cran.r-project.org\")\n",
    "if (!require(timeSeries)) install.packages(\"timeSeries\", repos=\"http://cran.r-project.org\")\n",
    "if (!require(Metrics)) install.packages(\"Metrics\", repos=\"http://cran.r-project.org\")\n",
    "\n",
    "library(fBasics)\n",
    "library(timeDate)\n",
    "library(timeSeries)\n",
    "library(Metrics)\n",
    "\"\"\")\n",
    "\n",
    "pandas2ri.activate()\n",
    "\n",
    "def compute_long_run_volatility(title):\n",
    "    df = pd.read_csv(title_filename.format(title=title))\n",
    "    \n",
    "    # Prendo come training set tutti i dati fino al 31 ottobre 2024\n",
    "    r_data_training = df[df['Date'] <= '2024-10-31']['log_ret']\n",
    "    r_data_testing= df[df['Date'] > '2024-10-31']['log_ret']\n",
    "\n",
    "\n",
    "    # Converti la Serie Pandas in un DataFrame per facilitarne la conversione in R\n",
    "    r_data_training = pd.DataFrame(r_data_training, columns=[\"log_ret\"])\n",
    "    r_data_testing = pd.DataFrame(r_data_testing, columns=[\"log_ret\"])\n",
    "\n",
    "    # Converti il DataFrame Pandas in un oggetto R\n",
    "    r_data_training = pandas2ri.py2rpy(r_data_training)\n",
    "    r_data_testing = pandas2ri.py2rpy(r_data_testing)\n",
    "\n",
    "\n",
    "    # Passa il dato a R\n",
    "    ro.globalenv['train_log_rets'] = r_data_training\n",
    "    ro.globalenv['test_log_rets'] = r_data_testing\n",
    "    \n",
    "\n",
    "    # Scrivi lo script per calcolare il modello GARCH con la serie reale\n",
    "    r_script = \"\"\"\n",
    "    invisible(capture.output(suppressMessages({ #Comment this to have R prints\n",
    "    # Fit GARCH(1,1) con i dati reali\n",
    "    garch_model <- fGarch::garchFit(formula=~garch(1,1), data=train_log_rets, init.rec=\"mci\", cond.dist=\"norm\", algorithm=\"lbfgsb\")\n",
    "    #summary(garch_model)\n",
    "        \n",
    "    #Evaluate autocorrelation of model residuals with Ljung-Box test\n",
    "    #residuals <- residuals(model, standardize = TRUE)\n",
    "\n",
    "    # Test di Ljung-Box sui residui\n",
    "    #ljung_box_residuals <- Box.test(residuals, lag = 10, type = \"Ljung-Box\")\n",
    "    \n",
    "    #Evaluate accuracy of the model\n",
    "    forecast_length <- nrow(test_log_rets)\n",
    "    train_length <- nrow(train_log_rets)\n",
    "    \n",
    "    # Calcola la volatilità condizionata fittando il modello sul testing set\n",
    "    forecasts <- fGarch::predict(garch_model, n.ahead=forecast_length)\n",
    "    \n",
    "    # Extract forecasted means and conditional variances\n",
    "    forecasted_means <- forecasts$meanForecast\n",
    "    #forecasted_variances <- forecasts$standardDeviation^2\n",
    "    #forecasted_means\n",
    "    \n",
    "    #Convert test_log_rets and train_log_rets to numeric vector\n",
    "    test_log_rets <- test_log_rets$log_ret  \n",
    "    train_log_rets <- train_log_rets$log_ret  \n",
    "\n",
    "    \n",
    "    # Evaluate the model with MAE, MSE, RMSE, MPE, MAPE, SMAPE, MASE, RMMSE\n",
    "    MAE <- sum(abs(test_log_rets-forecasted_means))/forecast_length #Mean Absolute Error\n",
    "    MSE <- sum((test_log_rets-forecasted_means)^2)/forecast_length #Mean Squared Error\n",
    "    RMSE <- sqrt(sum((test_log_rets-forecasted_means)^2)/forecast_length) #Root Mean Squared Error\n",
    "    \n",
    "    MPE <- 100*sum(((test_log_rets - forecasted_means) / test_log_rets))/forecast_length #Mean Percentage Error\n",
    "    MAPE <- 100*sum(abs(((test_log_rets - forecasted_means) / test_log_rets)))/forecast_length #Mean Percentage Error\n",
    "    \n",
    "    SMAPE <- 100*sum(abs(test_log_rets - forecasted_means)/(abs(test_log_rets)+abs(forecasted_means)))/forecast_length #Symmetric Mean Absolute Percentage Error\n",
    "    \n",
    "    mase_num <- sum(abs(test_log_rets - forecasted_means))/forecast_length #Use test set at numerator\n",
    "    mase_den <- sum(abs(diff(train_log_rets)))/(train_length-1) #Use training set at denominator\n",
    "    \n",
    "    MASE <- mase_num / mase_den #Mean Absolute Scaled Error\n",
    "    \n",
    "    rmsse_numerator <- sum((test_log_rets - forecasted_means)^2)/forecast_length\n",
    "    rmsse_denominator <- sum(diff(train_log_rets)^2)/(train_length - 1)\n",
    "    \n",
    "    RMSSE <- sqrt(rmsse_numerator / rmsse_denominator) #Root Mean Squared Scaled Error\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Extract coefficients of the model\n",
    "    coefficients <- coef(garch_model)  \n",
    "    \n",
    "    }))) #comment this to have R prints\n",
    "    \"\"\"\n",
    "\n",
    "    # Esecuzione del codice in R\n",
    "    ro.r(r_script)\n",
    "    \n",
    "    #Print ljung box test results\n",
    "    #print(ro.r('ljung_box_residuals'))\n",
    "    \n",
    "    #Returns mu, omega, alpha1, beta1\n",
    "    coefficients = ro.r('coefficients')\n",
    "    MU = coefficients[0]\n",
    "    OMEGA = coefficients[1]\n",
    "    ALPHA1 = coefficients[2]\n",
    "    BETA1 = coefficients[3]\n",
    "    \n",
    "    long_run_volatility = OMEGA/(1-ALPHA1-BETA1)\n",
    "    \n",
    "    '''print(\"MU -> \" + str(MU))\n",
    "    print(\"OMEGA -> \" + str(OMEGA))\n",
    "    print(\"ALPHA1 -> \" + str(ALPHA1))\n",
    "    print(\"BETA1 -> \" + str(BETA1))\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"LONG RUN VOLATILITY -> \" + str(long_run_volatility))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    #Returns MAE, MAPE, RMSE, MSE, MPE, SMAPE, MASE, RMMSE\n",
    "    print(\"MAE -> \" + str(ro.r('MAE')[0]))\n",
    "    print(\"RMSE -> \" + str(ro.r('RMSE')[0]))\n",
    "    print(\"MSE -> \" + str(ro.r('MSE')[0]))\n",
    "    \n",
    "    print(\"MPE -> \" + str(ro.r('MPE')[0]))\n",
    "    print(\"MAPE -> \" + str(ro.r('MAPE')[0]))    \n",
    "    print(\"SMAPE -> \" + str(ro.r('SMAPE')[0]))\n",
    "    \n",
    "    print(\"MASE -> \" + str(ro.r('MASE')[0]))\n",
    "    print(\"RMSSE -> \" + str(ro.r('RMSSE')[0]))'''\n",
    "    return long_run_volatility\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=compute_long_run_volatility('^SPX')\n",
    "\n",
    "'''\n",
    "OMEGA -> 9.596220100948595e-07\n",
    "ALPHA1 -> 0.07713687415236392\n",
    "BETA1 -> 0.9149867274327818\n",
    "\n",
    "LONG RUN VOLATILITY -> 0.00012183512813230487\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Standardised Residuals Tests:\n",
    "                                 Statistic     p-Value\n",
    " Jarque-Bera Test   R    Chi^2   7.7015681 0.021263058 -> H0 è che i residui siano normali, la rigettiamo perche p<0.05\n",
    " Shapiro-Wilk Test  R    W       0.9935248 0.003018532 -> H0 è che i residui siano normali, la rigettiamo perche p<0.05\n",
    " Ljung-Box Test     R    Q(10)   6.2869703 0.790604543 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(15)  10.9893952 0.753345768 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(20)  23.0590353 0.285905087 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(10)  14.2173008 0.163308600 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(15)  19.2041441 0.204631699 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(20)  21.6649474 0.358964724 -> H0 è che i quadrati residui siano scorrelati, la accettiamo perche p>0.05\n",
    " LM Arch Test       R    TR^2   15.2297226 0.229113370 -> H0 è che i i residui siano eteroschedastici, la accettiamo perche p>0.05\n",
    "'''\n",
    "\n",
    "'''\n",
    "Accuracy:\n",
    "MAE -> 0.006094533709048875 \n",
    "RMSE -> 0.008610649261663116\n",
    "MSE -> 7.414328070737957e-05\n",
    "MPE -> -87.3943129139289\n",
    "MAPE -> 252.40103117705092\n",
    "SMAPE -> 78.40195738620056 \n",
    "MASE -> 0.5163388319803032 -> The model is better than a naive model case MASE < 1\n",
    "RMSSE -> 0.5508407413022812 -> The model is better than a naive model case RMSSE < 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_long_run_volatility('^NDX')\n",
    "\n",
    "'''\n",
    "OMEGA -> 1.5333717379024792e-06\n",
    "ALPHA1 -> 0.05178665160818798\n",
    "BETA1 -> 0.9405498291075637\n",
    "\n",
    "LONG RUN VOLATILITY -> 0.00020008715069774548\n",
    "'''\n",
    "\n",
    "'''\n",
    " Jarque-Bera Test   R    Chi^2   5.1105253 0.07767183 -> H0 è che i residui siano normali, la accettiamo perche p>0.05\n",
    " Shapiro-Wilk Test  R    W       0.9961994 0.07445377 -> H0 è che i residui siano normali, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(10)   7.3217410 0.69476141 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(15)  12.8546429 0.61352425 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(20)  21.2638981 0.38174870 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(10)  15.1731763 0.12587456 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(15)  20.7659377 0.14447586 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(20)  24.2607337 0.23118830 -> H0 è che i quadrati residui siano scorrelati, la accettiamo perche p>0.05\n",
    " LM Arch Test       R    TR^2   18.1407399 0.11148352 -> H0 è che i i residui siano eteroschedastici, la accettiamo perche p>0.05\n",
    "'''\n",
    "\n",
    "'''\n",
    "Accuracy:\n",
    "MAE -> 0.008033379728990257\n",
    "RMSE -> 0.011390500807927877\n",
    "MSE -> 0.00012974350865540562\n",
    "MPE -> 105.20046753081294\n",
    "MAPE -> 107.65264495006166\n",
    "SMAPE -> 79.56846554460454\n",
    "MASE -> 0.48238686730508334\n",
    "RMSSE -> 0.5221030090360853\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_long_run_volatility('^RUT')\n",
    "\n",
    "'''\n",
    "OMEGA -> 4.048774895259176e-06\n",
    "ALPHA1 -> 0.051128352743122475\n",
    "BETA1 -> 0.9294291094425949\n",
    "\n",
    "LONG RUN VOLATILITY -> 0.00020824312823426364\n",
    "'''\n",
    "\n",
    "'''\n",
    "Standardised Residuals Tests:\n",
    "                                Statistic   p-Value\n",
    " Jarque-Bera Test   R    Chi^2   2.980368 0.2253312 -> H0 è che i residui siano normali, la accettiamo perche p>0.05\n",
    " Shapiro-Wilk Test  R    W       0.996578 0.1176416 -> H0 è che i residui siano normali, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(10)   4.459829 0.9242289 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(15)   9.566764 0.8460712 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(20)  22.830264 0.2972056 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(10)  11.275451 0.3364644 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(15)  14.676889 0.4749321 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(20)  20.998310 0.3972321 -> H0 è che i quadrati residui siano scorrelati, la accettiamo perche p>0.05\n",
    " LM Arch Test       R    TR^2   15.590459 0.2107218 -> H0 è che i i residui siano eteroschedastici, la accettiamo perche p>0.05\n",
    "'''\n",
    "\n",
    "'''\n",
    "Accuracy:\n",
    "MAE -> 0.01202009832952726\n",
    "RMSE -> 0.016753480177713713\n",
    "MSE -> 0.00028067909806504625\n",
    "MPE -> 96.82370051310005\n",
    "MAPE -> 96.82370051310005\n",
    "SMAPE -> 94.08475196102255\n",
    "MASE -> 0.7412907425668092\n",
    "RMSSE -> 0.8125449331795127\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_long_run_volatility('NVDA')\n",
    "\n",
    "'''\n",
    "OMEGA -> 7.515112948238467e-05\n",
    "ALPHA1 -> 0.03359987556656807\n",
    "BETA1 -> 0.9050285576943499\n",
    "\n",
    "LONG RUN VOLATILITY -> 0.0012245268204066823\n",
    "'''\n",
    "\n",
    "'''\n",
    "Jarque-Bera Test   R    Chi^2  528.9313363 0.000000e+00 -> H0 è che i residui siano normali, la rigettiamo perche p<0.05 STRANO???\n",
    " Shapiro-Wilk Test  R    W        0.9679727 1.432350e-11 -> H0 è che i residui siano normali, la rigettiamo perche p<0.05\n",
    " Ljung-Box Test     R    Q(10)    8.1908486 6.102012e-01 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(15)   14.0810954 5.193878e-01 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(20)   19.5979460 4.833191e-01 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(10)    2.0114543 9.962516e-01 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(15)    3.6594247 9.986565e-01 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(20)    4.3375827 9.999101e-01 -> H0 è che i quadrati residui siano scorrelati, la accettiamo perche p>0.05\n",
    " LM Arch Test       R    TR^2     2.0699933 9.992908e-01 -> H0 è che i i residui siano eteroschedastici, la accettiamo perche p>0.05\n",
    "'''\n",
    "\n",
    "'''\n",
    "Accuracy:\n",
    "MAE -> 0.02115490886540065\n",
    "RMSE -> 0.025763244589133056\n",
    "MSE -> 0.0006637447717594937\n",
    "MPE -> 93.87573217972553\n",
    "MAPE -> 93.87573217972553\n",
    "SMAPE -> 79.77839923826744\n",
    "MASE -> 0.5576577774754948\n",
    "RMSSE -> 0.5129152355520931\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_long_run_volatility('JNJ')\n",
    "\n",
    "'''\n",
    "OMEGA -> 1.2719627293334906e-05\n",
    "ALPHA1 -> 0.006837376415740562\n",
    "BETA1 -> 0.874687992060343\n",
    "\n",
    "LONG RUN VOLATILITY -> 0.0001073616109180909\n",
    "'''\n",
    "\n",
    "'''\n",
    "Standardised Residuals Tests:\n",
    "                                  Statistic      p-Value\n",
    " Jarque-Bera Test   R    Chi^2  332.3234645 0.000000e+00 -> H0 è che i residui siano normali, la rigettiamo perche p<0.05\n",
    " Shapiro-Wilk Test  R    W        0.9665474 6.909536e-12 -> H0 è che i residui siano normali, la rigettiamo perche p<0.05\n",
    " Ljung-Box Test     R    Q(10)    9.5335088 4.823270e-01 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(15)   11.9041086 6.862723e-01 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(20)   14.1130944 8.247158e-01 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(10)   13.4230151 2.009725e-01 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(15)   17.0933117 3.133169e-01 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(20)   19.2805051 5.036658e-01 -> H0 è che i quadrati residui siano scorrelati, la accettiamo perche p>0.05\n",
    " LM Arch Test       R    TR^2    15.9016300 1.957832e-01 -> H0 è che i i residui siano eteroschedastici, la accettiamo perche p>0.05\n",
    "'''\n",
    "\n",
    "'''\n",
    "Accuracy:\n",
    "MAE -> 0.006185113818306013\n",
    "RMSE -> 0.007859111563955272\n",
    "MSE -> 6.176563457469547e-05\n",
    "MPE -> 102.94998142322815\n",
    "MAPE -> 102.94998142322815\n",
    "SMAPE -> 95.8306246221737\n",
    "MASE -> 0.5801644534792725\n",
    "RMSSE -> 0.537808128073258\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_long_run_volatility('XOM')\n",
    "\n",
    "'''\n",
    "OMEGA -> 8.603457755518873e-07\n",
    "ALPHA1 -> 0.024873336478831613\n",
    "BETA1 -> 0.9715906060158603\n",
    "\n",
    "LONG RUN VOLATILITY -> 0.0002433065000386456\n",
    "'''\n",
    "\n",
    "'''\n",
    "Standardised Residuals Tests:\n",
    "                                 Statistic      p-Value\n",
    " Jarque-Bera Test   R    Chi^2  20.8895352 2.910014e-05 -> H0 è che i residui siano normali, la rigettiamo perche p<0.05\n",
    " Shapiro-Wilk Test  R    W       0.9936667 3.555295e-03 -> H0 è che i residui siano normali, la rigettiamo perche p<0.05\n",
    " Ljung-Box Test     R    Q(10)  11.7915249 2.992513e-01 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(15)  20.8009089 1.433119e-01 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R    Q(20)  27.9673765 1.101739e-01 -> H0 è che i residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(10)   7.7976889 6.485908e-01 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(15)  15.2227726 4.354927e-01 -> H0 è che i quadrati dei residui siano scorrelati, la accettiamo perche p>0.05\n",
    " Ljung-Box Test     R^2  Q(20)  19.4795547 4.908793e-01 -> H0 è che i quadrati residui siano scorrelati, la accettiamo perche p>0.05\n",
    " LM Arch Test       R    TR^2   11.3263406 5.011776e-01 -> H0 è che i i residui siano eteroschedastici, la accettiamo perche p>0.05\n",
    "'''\n",
    "\n",
    "'''\n",
    "Accuracy:\n",
    "MAE -> 0.008742229965304485\n",
    "RMSE -> 0.011766014883476405\n",
    "MSE -> 0.00013843910623818827\n",
    "MPE -> 103.7901895069331\n",
    "MAPE -> 105.59375770067393\n",
    "SMAPE -> 79.74877850157623\n",
    "MASE -> 0.48024029333829615\n",
    "RMSSE -> 0.4878412107149021\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Calibrazione del modello</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calibrate_model(title):\n",
    "    risk_free_rate_df = pd.read_csv('./data/bond/daily-treasury-rates_mean.csv')\n",
    "    #/100 to get pure number and /12 to get monthly rate cause the value from treasury bills is annualized\n",
    "    r = risk_free_rate_df['4 WEEKS BANK DISCOUNT'][0]/100/12\n",
    "    \n",
    "    sigma = compute_long_run_volatility(title)\n",
    "\n",
    "    u_crr=(1/(2*(1+r)))*(1 + sigma**2 + (1+r)**2 + sqrt((r**2+sigma**2)*(r**2+4*r+sigma**2+4)))\n",
    "    u_jarrow_rudd=1+r+sigma\n",
    "    d_crr=(1/(2*(1+r)))*(1 + sigma**2 + (1+r)**2 - sqrt((r**2+sigma**2)*(r**2+4*r+sigma**2+4))) #or simply 1/u_crr\n",
    "    d_jarrow_rudd=1+r-sigma\n",
    "    p_crr=(1+r-d_crr)/(u_crr-d_crr)\n",
    "    p_jarrow_rudd=(1+r-d_jarrow_rudd)/(u_jarrow_rudd-d_jarrow_rudd)\n",
    "    q_crr=1-p_crr\n",
    "    q_jarrow_rudd=1-p_jarrow_rudd\n",
    "    \n",
    "    '''print(\"Parameters for \", title)\n",
    "    print(\"r -> \", r)\n",
    "    print(\"sigma -> \", sigma)\n",
    "    print(\"u_crr -> \", u_crr)\n",
    "    print(\"u_jarrow_rudd -> \", u_jarrow_rudd)\n",
    "    print(\"d_crr -> \", d_crr)\n",
    "    print(\"d_jarrow_rudd -> \", d_jarrow_rudd)\n",
    "    print(\"p_crr -> \", p_crr)\n",
    "    print(\"p_jarrow_rudd -> \", p_jarrow_rudd)\n",
    "    print(\"q_crr -> \", q_crr)\n",
    "    print(\"q_jarrow_rudd -> \", q_jarrow_rudd)\n",
    "    print()\n",
    "    print()'''\n",
    "    \n",
    "    return {'u_crr': u_crr, \n",
    "            'u_jarrow_rudd' :u_jarrow_rudd, \n",
    "            'd_crr': d_crr, \n",
    "            'd_jarrow_rudd': d_jarrow_rudd, \n",
    "            'p_crr': p_crr, \n",
    "            'p_jarrow_rudd': p_jarrow_rudd, \n",
    "            'q_crr': q_crr, \n",
    "            'q_jarrow_rudd': q_jarrow_rudd, \n",
    "            'sigma': sigma, \n",
    "            'r': r}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "title_filename = './data/title/{title}.csv'\n",
    "\n",
    "def predict_stock_price(title, first_day, last_day, model):\n",
    "    title_df = pd.read_csv(title_filename.format(title=title))\n",
    "    #take only the close price of the day equal to first_day, first day is yyyy_mm_dd and title_df['Date'] is yyyy-mm-dd hh:mm:ss  \n",
    "    s_0 = title_df[title_df['Date'].str.startswith(first_day)]['Close'].values[0]\n",
    "    \n",
    "    real_S_T = title_df[title_df['Date'].str.startswith(last_day)]['Close'].values[0]\n",
    "    \n",
    "    crr_predict_S_T = s_0*(model['u_crr']*model['p_crr'] + model['d_crr']*model['q_crr'])\n",
    "    \n",
    "    jarrow_rudd_predict_S_T = s_0*(model['u_jarrow_rudd']*model['p_jarrow_rudd'] + model['d_jarrow_rudd']*model['q_jarrow_rudd'])\n",
    "    \n",
    "    return {'crr_predict_S_T': crr_predict_S_T, 'jarrow_rudd_predict_S_T': jarrow_rudd_predict_S_T, 'real_S_T': real_S_T, 'S_0': s_0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "opt_filename_proc = './data/options_daily/proc/{date_dir}/{date_file}_{title}_{type}.csv'\n",
    "\n",
    "def predict_options_price(title, first_day, last_day):\n",
    "    model_params = calibrate_model(title)\n",
    "    print(\"MODEL PARAMETERS\")\n",
    "    print(model_params)\n",
    "    print()\n",
    "    print()\n",
    "    stock_predictions = predict_stock_price(title, first_day, last_day, model_params)\n",
    "    print(\"STOCK PREDICTIONS\")\n",
    "    print(stock_predictions)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    first_day = first_day.replace('-', '_')\n",
    "    df_C_0 = pd.read_csv(opt_filename_proc.format(date_dir=first_day, date_file=first_day, title=title, type='calls'))\n",
    "    df_P_0 = pd.read_csv(opt_filename_proc.format(date_dir=first_day, date_file=first_day, title=title, type='puts'))\n",
    "    first_day = first_day.replace('_', '-')\n",
    "    \n",
    "    #Filter df_C_0 and df_P_0 to only the one with expiration date equal to last_day\n",
    "    df_C_0 = df_C_0[df_C_0['expiration_date'] == last_day]\n",
    "    df_P_0 = df_P_0[df_P_0['expiration_date'] == last_day]\n",
    "    \n",
    "    last_day = last_day.replace('-', '_')\n",
    "    df_real_C_T = pd.read_csv(opt_filename_proc.format(date_dir=last_day, date_file=last_day, title=title, type='calls'))\n",
    "    df_real_P_T = pd.read_csv(opt_filename_proc.format(date_dir=last_day, date_file=last_day, title=title, type='puts'))\n",
    "    last_day = last_day.replace('_', '-')\n",
    "    \n",
    "    #Filter df_C_T and df_P_T to only the one with expiration date equal to last_day\n",
    "    df_real_C_T = df_real_C_T[df_real_C_T['expiration_date'] == last_day]\n",
    "    df_real_P_T = df_real_P_T[df_real_P_T['expiration_date'] == last_day]\n",
    "    \n",
    "    #Join the datasets on contract_symbol and take only the columns contract_symbol, strike, lastPrice, expiration_date\n",
    "    df_join_C = pd.merge(df_C_0, df_real_C_T, on='contractSymbol', suffixes=('_0', '_T'))[['contractSymbol', 'strike_0', 'lastPrice_0', 'lastPrice_T', 'expiration_date_0']]\n",
    "    df_join_P = pd.merge(df_P_0, df_real_P_T, on='contractSymbol', suffixes=('_0', '_T'))[['contractSymbol', 'strike_0', 'lastPrice_0', 'lastPrice_T', 'expiration_date_0']]\n",
    "    \n",
    "    #rename columns strike_0 in strike and expiration_date_0 in expiration_date\n",
    "    df_join_C = df_join_C.rename(columns={'strike_0': 'strike', 'expiration_date_0': 'expiration_date'})\n",
    "    df_join_P = df_join_P.rename(columns={'strike_0': 'strike', 'expiration_date_0': 'expiration_date'})\n",
    "    \n",
    "    df_join_C['lastPrice_CRR'] = np.maximum(stock_predictions['crr_predict_S_T']-df_join_C['strike'], 0)\n",
    "    df_join_P['lastPrice_CRR'] = np.maximum(df_join_P['strike']-stock_predictions['crr_predict_S_T'], 0)\n",
    "    \n",
    "    df_join_C['lastPrice_JR'] = np.maximum(stock_predictions['jarrow_rudd_predict_S_T']-df_join_C['strike'], 0)\n",
    "    df_join_P['lastPrice_JR'] = np.maximum(df_join_P['strike']-stock_predictions['jarrow_rudd_predict_S_T'], 0)\n",
    "    \n",
    "    print(df_join_C)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL PARAMETERS\n",
      "{'u_crr': np.float64(1.0037803122707087), 'u_jarrow_rudd': np.float64(1.003982804531743), 'd_crr': np.float64(0.9962339246700735), 'd_jarrow_rudd': np.float64(1.0035663182752745), 'p_crr': np.float64(0.9992379311129578), 'p_jarrow_rudd': np.float64(0.5), 'q_crr': np.float64(0.0007620688870422221), 'q_jarrow_rudd': np.float64(0.5), 'sigma': np.float64(0.00020824312823426364), 'r': np.float64(0.0037745614035087725)}\n",
      "\n",
      "\n",
      "STOCK PREDICTIONS\n",
      "{'crr_predict_S_T': np.float64(2444.170961921344), 'jarrow_rudd_predict_S_T': np.float64(2444.170961921344), 'real_S_T': np.float64(2426.18994140625), 'S_0': np.float64(2434.97998046875)}\n",
      "\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [contractSymbol, strike, lastPrice_0, lastPrice_T, expiration_date, lastPrice_CRR, lastPrice_JR]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "first_day = '2024-11-11'\n",
    "last_day = '2024-11-27'\n",
    "\n",
    "#DOMANDA Vogliamo solo opzioni che esistono l'11-11 e scadono il 27-11 \n",
    "#o ci stanno bene anche titoli creati dopo ma che scadono 2l 17-11?? \n",
    "# Io penso solo opzioni che partono da 11-11 altrimenti cambia s_0 e anche il tasso risk fre\n",
    "european = ['^SPX', '^NDX', '^RUT'] #RUT ci sono titoli solo a partire dal 18 che scadono il 27 \n",
    "american = ['NVDA', 'JNJ', 'XOM']\n",
    "\n",
    "#for title in european+american:\n",
    "for title in ['^RUT']:\n",
    "    predict_options_price(title, first_day, last_day)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Put-Call Parity Equation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import ast \n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "european = ['^SPX', '^NDX', '^RUT']\n",
    "#european = ['^NDX']\n",
    "\n",
    "american = ['NVDA', 'JNJ', 'XOM']\n",
    "\n",
    "#Parametric string\n",
    "opt_filename = './data/options_daily/raw/{date_dir}/{date_file}_{title}_{type}.csv'\n",
    "\n",
    "opt_filename_proc = './data/options_daily/proc/{date_dir}/{date_file}_{title}_{type}.csv'\n",
    "\n",
    "title_filename = './data/title/{title}.csv'\n",
    "\n",
    "#List of dates day by day from 2024_11_12 to 2024_11_29 \n",
    "dates = pd.date_range(start='2024-11-11', end='2024-11-29').strftime('%Y_%m_%d').tolist()\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Import library in R\n",
    "ro.r(\"\"\"\n",
    "if (!require(ggplot2)) install.packages(\"ggplot2\", repos=\"http://cran.r-project.org\")\n",
    "library(ggplot2)\n",
    "if (!require(grid)) install.packages(\"grid\", repos=\"http://cran.r-project.org\")\n",
    "library(grid)\n",
    "if (!require(boot)) install.packages(\"boot\", repos=\"http://cran.r-project.org\")\n",
    "library(boot)\n",
    "\"\"\")\n",
    "\n",
    "pandas2ri.activate()\n",
    "\n",
    "def put_call_parity(date, title, risk_free_rate, s_0):\n",
    "    \n",
    "    p_0_df = pd.read_csv(opt_filename_proc.format(date_dir=date, date_file=date, title=title, type=\"puts\"))\n",
    "    c_0_df = pd.read_csv(opt_filename_proc.format(date_dir=date, date_file=date, title=title, type=\"calls\"))\n",
    "    \n",
    "    p_0_df = p_0_df[['lastPrice', 'strike', 'expiration_date']]\n",
    "    c_0_df = c_0_df[['lastPrice', 'strike', 'expiration_date']]\n",
    "    \n",
    "    put_call_df = pd.merge(\n",
    "    p_0_df,\n",
    "    c_0_df,\n",
    "    on=['strike', 'expiration_date'],\n",
    "    suffixes=('_put', '_call')\n",
    "    )\n",
    "\n",
    "    #take expiration last date\n",
    "    put_call_df = put_call_df[put_call_df['expiration_date'] =='2024-11-29'] \n",
    "\n",
    "    put_call_df = put_call_df[['lastPrice_put', 'strike', 'lastPrice_call']]\n",
    "    \n",
    "    # Converti il DataFrame Pandas in un oggetto R\n",
    "    put_call_df = pandas2ri.py2rpy(put_call_df)\n",
    "\n",
    "\n",
    "    # Passa il dato a R\n",
    "    ro.globalenv['title'] = title\n",
    "    ro.globalenv['put_call_df'] = put_call_df\n",
    "    ro.globalenv['risk_free_rate'] = risk_free_rate\n",
    "    ro.globalenv['s_0'] = s_0\n",
    "\n",
    "    # Scrivi lo script per calcolare il modello GARCH con la serie reale\n",
    "    r_script = \"\"\"\n",
    "        N <- 19 #numero di giorni osservati\n",
    "        x <-  s_0 - put_call_df$strike/(1+risk_free_rate) \n",
    "        y <- put_call_df$lastPrice_call - put_call_df$lastPrice_put\n",
    "        Data_df <- data.frame(x,y)\n",
    "        \n",
    "        #Confidence interval calculation\n",
    "        put_call_lm <- lm(y ~ x)\n",
    "        lm_residuals <- put_call_lm[[\"residuals\"]]\n",
    "        \n",
    "        #Compute bootstrap quantiles\n",
    "        y <- lm_residuals\n",
    "        d <- data.frame(k=1:length(y), y=y)\n",
    "        \n",
    "        #Function to compute the quantiles, to have a 80perc confidence interval we take the 10th and 90th quantile so the remaining is 80perc\n",
    "        boot_quant <- function(d, k){\n",
    "            d2 <- d[k,]\n",
    "            return(quantile(d2$y, probs=c(0.1,0.9))) \n",
    "        }\n",
    "\n",
    "        #Compute the confidence interval\n",
    "        set.seed(12345)\n",
    "        booted_quant <- boot(d, boot_quant, R=5000)\n",
    "\n",
    "        #Compute confidence intervals of the residuals as mean of the quantiles\n",
    "        ci_low <- mean(booted_quant$t[,1])\n",
    "        ci_up <- mean(booted_quant$t[,2])\n",
    "\n",
    "        #calculate the number of residuals whose are in the confidence interval\n",
    "        residuals_in_ci <- sum(lm_residuals >= ci_low & lm_residuals <= ci_up)\n",
    "        \n",
    "        #calculate the percentage\n",
    "        residuals_in_ci_perc <- 100*residuals_in_ci/length(lm_residuals)\n",
    "        \n",
    "        \n",
    "        \n",
    "        n <- nrow(Data_df)\n",
    "        title_content <- bquote(atop(\"University of Roma Tor Vergata - \\u0040 MPSMF 2023-2024\", \n",
    "                                    paste(\"Scatter Plot of the Call-Put Difference Against the Strike Price for \" ~ .(title))))\n",
    "        subtitle_content <- bquote(paste(\"Data set size\",~~.(n),~~\"sample points;    Evaluation Date 2024-11-27;   Maturity Date 2024-11-27\"))\n",
    "        caption_content <- \"Author: Matteo Conti\" \n",
    "        \n",
    "\n",
    "        x_breaks_num <- 8\n",
    "        x_breaks_low <- min(Data_df$x)\n",
    "        x_breaks_up <- max(Data_df$x)\n",
    "        x_binwidth <- floor((x_breaks_up-x_breaks_low)/x_breaks_num)\n",
    "        x_binwidth <- ifelse(x_binwidth == 0, 1, x_binwidth)\n",
    "        \n",
    "        x_breaks <- seq(from=x_breaks_low, to=x_breaks_up, by=x_binwidth)\n",
    "        \n",
    "        if((x_breaks_up-max(x_breaks))>x_binwidth/2){x_breaks <- c(x_breaks,x_breaks_up)}\n",
    "        x_labs <- format(x_breaks, scientific=FALSE)\n",
    "        J <- 0.2\n",
    "        x_lims <- c(x_breaks_low-J*x_binwidth,x_breaks_up+J*x_binwidth)\n",
    "        x_name <- bquote(\"strike\")\n",
    "        y_breaks_num <- 10\n",
    "        y_max <- max(Data_df$y)\n",
    "        y_min <- min(Data_df$y)\n",
    "        y_binwidth <- round((y_max-y_min)/y_breaks_num, digits=3)\n",
    "        y_breaks_low <- y_min\n",
    "        y_breaks_up <- y_max\n",
    "        y_breaks <- seq(from=y_breaks_low, to=y_breaks_up, by=y_binwidth)\n",
    "        if((y_breaks_up-max(y_breaks))>y_binwidth/2){y_breaks <- c(y_breaks,y_breaks_up)}\n",
    "        y_labs <- format(y_breaks, scientific=FALSE)\n",
    "        y_name <- bquote(\"call-put difference\")\n",
    "        K <- 0.2\n",
    "        y_lims <- c((y_breaks_low-K*y_binwidth), (y_breaks_up+K*y_binwidth))\n",
    "        \n",
    "        col_1 <- bquote(\"data set sample points\")\n",
    "        col_2 <- bquote(\"regression line\")\n",
    "        col_3 <- bquote(\"LOESS curve\")\n",
    "        col_4 <- bquote(\"80%% Confidence interval\")\n",
    "        \n",
    "        leg_labs <- c(col_1, col_2, col_3, col_4)\n",
    "        leg_cols <- c(\"col_1\"=\"blue\", \"col_2\"=\"green\", \"col_3\"=\"red\", \"col_4\"=\"purple\")\n",
    "        leg_ord <- c(\"col_1\", \"col_2\", \"col_3\", \"col_4\")\n",
    "        \n",
    "        Call_Put_Strike_Pr_2024_11_11_11_27_sp <- ggplot(Data_df, aes(x=x, y=y)) +\n",
    "        geom_smooth(alpha=1, linewidth=0.8, linetype=\"dashed\", aes(color=\"col_3\"),\n",
    "                    method=\"loess\", formula=y ~ x, se=FALSE, fullrange = FALSE) +\n",
    "        geom_smooth(alpha=1, linewidth=0.8, linetype=\"solid\", aes(color=\"col_2\"),\n",
    "                    method=\"lm\" , formula=y ~ x, se=FALSE, fullrange=FALSE) +\n",
    "        geom_abline(aes(intercept = coef(lm(y ~ x, data = Data_df))[1] + ci_up,\n",
    "            slope = coef(lm(y ~ x, data = Data_df))[2],\n",
    "            color = \"col_4\"), linetype = \"dotted\", linewidth = 0.8) +\n",
    "        geom_abline(aes(intercept = coef(lm(y ~ x, data = Data_df))[1] + ci_low,\n",
    "            slope = coef(lm(y ~ x, data = Data_df))[2],\n",
    "            color=\"col_4\"), linetype = \"dotted\", linewidth = 0.8) +\n",
    "        geom_point(alpha=1, size=1.0, shape=19, aes(color=\"col_1\")) +\n",
    "        scale_x_continuous(name=x_name, breaks=x_breaks, label=x_labs, limits=x_lims) +\n",
    "        scale_y_continuous(name=y_name, breaks=y_breaks, labels=NULL, limits=y_lims,\n",
    "                            sec.axis=sec_axis(~., breaks=y_breaks, labels=y_labs)) +\n",
    "        ggtitle(title_content) +\n",
    "        labs(subtitle=subtitle_content, caption=caption_content) +\n",
    "        scale_colour_manual(name=\"Legend\", labels=leg_labs, values=leg_cols) +\n",
    "        theme(plot.title=element_text(hjust=0.5), plot.subtitle=element_text(hjust=0.5),\n",
    "                axis.text.x=element_text(angle=0, vjust=1),\n",
    "                legend.key.width=unit(1.0,\"cm\"), legend.position=\"bottom\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Print the plot\n",
    "        #x11(width = 26.67, height = 15)\n",
    "        #plot(Call_Put_Strike_Pr_2024_11_11_11_27_sp)\n",
    "        \n",
    "        \n",
    "        # Construct the output file name dynamically\n",
    "        output_file <- paste0(\"./plots/Call_Put_Strike_Pr_2024_11_11_11_27_sp_\", title, \".pdf\")\n",
    "\n",
    "        # Save the plot to the dynamically generated file name\n",
    "        ggsave(filename = output_file, \n",
    "            plot = Call_Put_Strike_Pr_2024_11_11_11_27_sp, \n",
    "            width = 26.67, height = 15, units = \"cm\")        \n",
    "    \"\"\"\n",
    "\n",
    "    # Esecuzione del codice in R\n",
    "    ro.r(r_script)\n",
    "    print(\"PERC OF RESIDUALS IN CI -> \" + str(ro.r('residuals_in_ci_perc')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_day = '2024-11-11'\n",
    "\n",
    "#title = '^SPX'\n",
    "\n",
    "risk_free_rate_df = pd.read_csv('./data/bond/daily-treasury-rates_mean.csv')\n",
    "#/100 to get pure number and /12 to get monthly rate cause the value from treasury bills is annualized\n",
    "monthly_risk_free_rate = risk_free_rate_df['4 WEEKS BANK DISCOUNT'][0]/100/12\n",
    "\n",
    "\n",
    "\n",
    "for title in european+american:\n",
    "    print(\"\\n\\nPUT CALL FOR TITLE -> \" + title)\n",
    "    title_df = pd.read_csv(title_filename.format(title=title))\n",
    "\n",
    "    #take only the close price of the day equal to first_day, first day is yyyy_mm_dd and title_df['Date'] is yyyy-mm-dd hh:mm:ss  \n",
    "    s_0 = title_df[title_df['Date'].str.startswith(first_day)]['Close'].values[0]\n",
    "\n",
    "    #Convert first_day to yyyy_mm_dd\n",
    "    first_day = first_day.replace('-', '_')\n",
    "\n",
    "    put_call_parity(first_day, title, monthly_risk_free_rate, s_0)\n",
    "    \n",
    "    first_day = first_day.replace('_', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "^SPX PERC OF RESIDUALS IN CI -> [80.60836502]\n",
    "\n",
    "^NDX PERC OF RESIDUALS IN CI -> [80.39215686]\n",
    "\n",
    "^RUT PERC OF RESIDUALS IN CI -> [78.88888889]\n",
    "\n",
    "NVDA PERC OF RESIDUALS IN CI -> [81.08108108]\n",
    "\n",
    "JNJ PERC OF RESIDUALS IN CI -> [57.14285714]\n",
    "\n",
    "XOM PERC OF RESIDUALS IN CI -> [72.72727273]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
